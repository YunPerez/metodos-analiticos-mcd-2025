# Más detalles y diagnósticos del flujo bayesiano

```{r}
#| include: false
library(tidyverse)
library(kableExtra)
library(DiagrammeR)
ggplot2::theme_set(ggplot2::theme_light())
```


En esta parte veremos más detalles del flujo Bayesiano de trabajo, partiendo del
plantwamiento de [M. Betancourt](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow)

Veamos el diagrama que se propone en este artículo:

![Flujo bayesiano, Betancourt](./figuras/betancourt-flujo.png)

Este flujo está dividido en tres partres: una pre-modelo y pre-datos, una pre-datos,
y uno cuando ya tenemos modelo y datos. Este enfoque es robusto desde el punto de vista
estadístico y computacional, aunque está escrito de una forma un poco diferente a como
lo planteamos en secciones anteriores.

1. Pre-modelo y pre-datos: el análisis conceptual, definición de espacio de observaciones
y construcción de estadísticas resumen todos son basados en conocimiento de dominio y
deben tener una orientación causal. En esta parte es donde planteamos diagramas causales,
modelos generativos, y qué cantidades (con interpretación causal) que nos interesa estimar.

2. En la segunda parte, construimos modelos y definimos estimadores en función de 
estos modelos. Aquí es donde hacemos simulaciones para entender nuestra información 
a priori o inicial, y las consecuencias que tienen nuestras primeras decisiones de
modelación. 

- (Supuestos de modelación) Podemos calcular qué tipo de valores obtenemos para las cantidades que queremos
estimar, y si son consistentes con el conocimiento de dominio (chequeos a priori, que incluye
también simular las posibles observaciones resultantes que implica el modelo)
- Podemos usar estas simulaciones para verificar que el algoritmo de ajuste que usamos está
bien calibrado (suponiendo que el modelo es correcto)
- Podemos también usar estas simulaciones para hacer calibración inferencial: ¿el modelo
recupera los valores que usamos para simular? ¿Existe evidencia de posible sobre ajuste o
información a priori incorrecta?

3. En la tercera parte, ajustamos el modelo a los datos. Diagnosticamos el algoritmo
(diagnósticos de MCMC que vimos anterioremente), y podemos hacer también chequeos predictivos
posteriores para entender cómo se comporta el modelo con los datos reales.


Para introducir algunos análisis que consideraremos planteamos una situación simple,
donde estamos midiendo los resultados de 100 sensores de alguna partícula,
todos intentando medir la misma fuente fija, como un experimento de control de calidad.
En este caso, todos los detectores están
midiendo la misma cantidad, pero hay cierto error en la medición, y podemos escribir el diagrama simple,
que supone que dada la fuente las observaciones son observaciones independientes.
$L$ representa la fuente seleccionada, y las $y_j$ son las observaciones de los sensores:

```{r}
#| out-width: 100%
#| code-fold: true
grViz("
digraph {
  graph [ranksep = 0.3, rankdir = LR]
  node [shape=circle]
    L
  node [shape=plaintext]
    y_j
  edge [minlen = 3]
    L -> y_j    
}
")#, width = 200, height = 50)
```

Comenzaremos con un modelo simple. Como las observaciones son enteros, supondremos que las observaciones son Poisson
con media $\lambda_F$. Nos interesa entonces estimar $\lambda_F$ que nos da la intensidad
de la fuente.

Para poner una inicial necesitamos concocimiento de dominio. Supongamos que sabemos que
para este tipo de fuentes, detectores, y tiempo de detección es extremo obtener conteos mayores a 25 partículas: los pondremos en el 1% de la cola superior, por ejemplo. 
Podemos experimentar con valores para $\sigma$ en una normal truncada en cero.

```{r}
# valores de la inincial
lambda <- seq(3, 20, 2)
# simular observaciones
q_poisson <- lambda |> map_df(~ tibble(lambda = .x, q_99 = quantile(rpois(10000, .x), probs = 0.99)))
q_poisson
```
Y podemos ver que requerimos aproximadamente $\lambda\leq 99$ con alta probabilidad.
Experimentando, podemos ver que si $\sigma=6$ es un valor razonable para la normal truncada:

```{r}
quantile(abs(rnorm(10000, 0, 6)), probs = 0.99)
```

Ahora construimos nuestro modelo generativo y examinamos sus consecuencias. Simularemos
1500 repeticiones de las 100 observaciones que esperamos:

```{r}
library(cmdstanr)
N <- 100
R <- 1500
sim_datos <- list(N = N)

mod_ensemble <- cmdstan_model("src/flujo-mb/1-simular-ensemble.stan")
print(mod_ensemble)
sims_priori <- mod_ensemble$sample(data = sim_datos, 
  iter_sampling = R, chains = 1, refresh = R, seed = 4838282,
  fixed_param = TRUE)
```

Ahora podemos examinar algunas posibles configuraciones del modelo junto con las
observaciones que esperaríamos ver:

```{r}
#| warning: false
#| message: false
sims_tbl <-  sims_priori$draws(format = "df")
obs_priori_tbl <-  sims_tbl |> 
  as_tibble() |>
  pivot_longer(cols = starts_with("y"), names_to = "y", values_to = "valor") |> 
  separate(y, into = c("y", "n"), sep = "[\\[\\]]") |> select(-y)
ggplot(obs_priori_tbl |> filter(.draw < 5), aes(x = valor)) +
  geom_histogram(bins = 30) +
  facet_wrap(~lambda) +
  labs(subtitle = "Simulaciones de observaciones a priori")
```
Y podemos resumir estas simulaciones como sigue: que muestra la distribución predictiva
a priori:

```{r}
ggplot(obs_priori_tbl |> group_by(.draw, valor) |> count() |> 
         group_by(valor) |> 
         summarise(mediana = median(n), q_10 = quantile(n, 0.1), q90 = quantile(n, 0.9)) |> 
         pivot_longer(cols = c(mediana, q_10, q90), names_to = "tipo", values_to = "resumen"),
  aes(x = valor)) +
  geom_line(aes(y = resumen, group = tipo)) +
  labs(subtitle = "Distribución predictiva a priori") +
  ylab("Frecuencia")
```
Y vemos que es muy poco probable observar cantidades medidas por arriba de 25:

```{r}
obs_priori_tbl |> mutate(mayor_25 = valor > 25) |> 
  summarise(mayor_25 = mean(mayor_25)) 
```

## Calibración algorítmica

Bajo los supuestos del modelo, ahora podemos proponer nuestro algoritmo de estimación,
y checar en primer lugar que funciona apropiadamente. En este ejemplo, tomaremos
solamente 40 simulaciones y ajustaremos en cada caso el siguiente modelo 
consecuencia de nuestros supuestos. Usualmente podemos usar 100 o más.

En este paso podemos ajustar nuestro muestreador, número de cadenas y su longitud, etc. Hacemos
un ajuste para cada posible conjunto de observaciones, y extraemos también la $\lambda_F$ que
generó cada conjunto de datos:

```{r}
#| message: false
mod_2 <- cmdstan_model("src/flujo-mb/2-modelo-poisson.stan")
print(mod_2)
ajustes_apriori <- purrr::map(1:40, function(rep){
  y <- obs_priori_tbl |> filter(.draw == rep) |> pull(valor)
  datos_sim_lst <- list(y = y, N = length(y))
  # Ajustar modelo
  ajuste <- mod_2$sample(data = datos_sim_lst, 
  iter_sampling = 1000, chains = 3, parallel_chains = 3, seed = 4838282, 
    refresh = 0, show_messages = FALSE)
  list(ajuste = ajuste, lambda_sim = sims_tbl$lambda[rep])
})

```

Podemos checar por ejemplo tamaño efectivo de muestra, rhat o divergencias, para cada
caso que extrajimos de la inicial de $\lambda_F$:

```{r}
ajustes_apriori |> map_df(~ .x$ajuste$summary("lambda")) |> 
  ggplot(aes(x = ess_bulk)) + geom_histogram(bins = 30) 
```



```{r}
ajustes_apriori |> map_dbl(
  ~ .x$ajuste$diagnostic_summary("divergences")$num_divergent |> sum()) |> 
  sum()
```






Adicionalmente, podemos ver si recuperamos o no los parámetros de la simulación.
En primer lugar, calculamos el cuantil del valor verdadero para la posterior de la simulación:

```{r}
resumen_cuantiles_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  lambda_sim <- ajuste_lst$lambda_sim
  cuantiles_tbl <- ajuste_lst$ajuste$draws("lambda", format = "df") |> 
    mutate(menor = lambda_sim < lambda) |> 
    summarise(q_menor = mean(menor))
  cuantiles_tbl |> mutate(lambda_sim = lambda_sim)
})
```

Ahora podemos hacer una gráfica de cuantiles: si estamos recuperando correctamente
los parámetros, la distribución de los cuantiles de los valores verdaderos
en la posterior debe ser cercana a uniforme (pues si $y\sim F$, entonces 
$P(F(y)<t)) = t$ para cualquier $t$):

```{r}
resumen_cuantiles_tbl
ggplot(resumen_cuantiles_tbl, aes(sample = q_menor)) +
  geom_qq(distribution = stats::qunif) +
  geom_abline(slope = 1, intercept = 0) +
  labs(subtitle = "Recuperación de valores en la posterior")
```

En este caso, vemos que estamos recuperando adecuadamente los valores
que pusimos en la simulación. Además de esta gráfica de cuantiles, hay otras
alternativas que puedes ver [aquí](https://hyunjimoon.github.io/SBC/articles/rank_visualizations.html).

**Nota**: Esto es porque si $\lambda \sim F_{lambda}$, donde $F_{\lambda})$ es la función
de distribución acumulada, entonces $P(F(\lambda)<t)) = t$ para cualquier $t$. Entonces,
si encontramos qué cuantil es cada valor $\lambda_i$ en su posterior $p_i$, y lo denotamos
por $F_i(\lambda_i)$, entonces estos valores están entre cero y uno, y cada uno se
distribuye uniforme en $[0,1]$. Cada punto es una corrida de un modelo distinto.


## Calibración inferencial

Finalmente veremos si las posteriores obtenidas dan inferencias que
sean suficientes para nuestros propósitos. Queremos determinar, con el modelo planteado, tamaño de datos e iniciales:

- ¿Nuestro modelo tiene problemas para aprender más allá de la a priori ("identificación" en el
sentido estádistico)??
- ¿Nuestro modelo a priori es adecuado para los valores de las cantidades de interés
que queremos estimar? 
- Nuestro ajuste tiende a sobreajustar los datos y darnos malas inferencias?

Abajo presentamos una demostración de [M. Betancourt](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow#132_A_Bayesian_Eye_Chart) de cómo pueden verse estos problemas:

![Inferencia y calibración](figuras/inf-cal-betancourt.png)
Y podemos calcular, para un parámetro particular, dos valores útiles. Primero,
el valor z posterior de cada parámetro, que está dado por:

$$ z(\theta^*, y) = \frac{\theta^* - \mu_{post}({\theta}|y)}{\sigma_{post}(\theta|y)}$$    
donde $\theta^*$ es el valor verdadero. Esta es también una medida de qué tanto
está el valor verdadero en el centro de la distribución o en una cola de la posterior,
y mide, en cada simulación, con qué precisión recuperamos con la posterior el 
valor verdadero. Valores chicos indican que la posterior está altamente concentrada
en el valor verdadero. 

Igualmente necesitamos la *contracción posterior*, que podemos definir como

$$c(y) = 1 - \frac{Var_{post}(\theta|y)}{Var_{previa}(\theta)}$$

y esta cantidad mide qué tanto los datos informan sobre el parámetro con respecto
a la información previa. Contracciones cercanas a cero indican que no aprendimos mucho
por encima de lo que sabíamos con la inicial. 

Usualmente queremos que la contracción sea cercana a 1, y qué los valores $z$ estén cercanos
a cero. Sin embargo, podemos encontrar:

1. Sobreajuste: la contracción es cercana a 1, pero observamos valores $z$ grandes en valor absoluto
(más allá de 4 y -4 por ejemplo).
2. Información previa incorrecta: la contracción es cercana a 0, y observamos valores de $z$ grandes.
3. Indentificación pobre: aunque los valores de $z$ no son muy lejanos a 0, la contracción
es cercana a 0 (no aprendemos mucho de los datos).


Podemos hacer esta gráfica para nuestro ejemplo de arriba:

```{r}
contraccion_z_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  lambda_sim <- ajuste_lst$lambda_sim
  sd_previa <- 3.69 # ver inicial de lambda
  post_media_sd_tbl <- ajuste_lst$ajuste$draws("lambda", format = "df") |> 
    summarise(media_post = mean(lambda), sd_post = sd(lambda))
  tibble(contraccion = 1 - post_media_sd_tbl$sd_post^2/sd_previa^2,
         z = (lambda_sim - post_media_sd_tbl$media_post)/post_media_sd_tbl$sd_post)
})
```

```{r}
ggplot(contraccion_z_tbl, aes(x = contraccion, y = z)) +
  geom_point() +
  xlab("Contracción") + ylab("Valor z") + 
  xlim(0, 1)
```

Y en este caso, obtenemos resultados informativos (alta contracción), que según
los valores $z$ capturan adecuadamente los valores verdaderos.


::: callout-note
# Sobreajuste

Nótese que esta forma de ver el sobreajuste está más relacionada con la inferencia
acerca de parámetros de interés que al sobreajuste en modelos predictivos.

Aunque también con modelos bayesianos podemos hacer validación cruzada para 
predicciones (ya sea tradicional o con métodos computacionalmente más eficientes que
aproximan el desempeño predictivo), nuestro objetivo principal *no* es obtener buenas
predicciones, sino tener  inferencia correcta e informativa acerca de las cantidades
de interés.

:::

El punto de vista predictivo también es importante, y puedes ver el texto
de McElreath para más detalles (por ejemplo, el uso de validación cruzada adaptada
para modelos bayesianos).

Finalmente, vamos al ajuste de datos reales y diagnósticos asociados

## Datos reales: diagnóstico de ajuste

Ya hemos visto antes cómo hacer diagnósticos del ajuste (checar MCMC, divergencias, etc.):

```{r}
datos_obs <- read_csv("../datos/ejemplo-flujo.csv")
datos_lst <- list(y = datos_obs$y, N = nrow(datos_obs))
  ajuste <- mod_2$sample(data = datos_lst, 
  iter_sampling = 1000, refresh = 1000, chains = 3, parallel_chains = 3, seed = 282)
```

```{r}
ajuste$summary("lambda") |> 
select(mean, sd, q5, q95, rhat, ess_bulk, ess_tail)
```

Los diagnósticos no muestran problemas. 

## Chequeos predictivos posteriores

Ahora simulamos datos observados de la predictiva posterior ajustada, y comparamos con
los datos observados.

```{r}
sims_post_pred_tbl <- ajuste$draws("y_sim", format = "df") |> 
  as_tibble() |>
  pivot_longer(cols = starts_with("y_sim"), names_to = "y", values_to = "valor") |> 
  separate(y, into = c("y_sim", "n"), sep = "[\\[\\]]", extra = "drop") |> select(-y_sim)
```

```{r}
ggplot(sims_post_pred_tbl |> filter(.draw <=10) |> 
  bind_rows(datos_obs |> rename(valor = y) |> mutate(.draw = 11)), 
    aes(x = valor)) +
  geom_histogram(bins = 30) +
  facet_wrap(~.draw) +
  labs(subtitle = "Chequeo predictivo posterior")
```
Y vemos un desajuste claro en el modelo: los datos tienen exceso de ceros, y los
datos que nos son cero tienden a ser mayores que los simulados. En este punto,
es necesario regresar al análisis conceptual, pues hay algo fundamental en 
el proceso generador de datos que no estamos considerando.

Podemos hacer también hacer una gráfica agregada de la posterior predictiva,
comparando con la observada, donde vemos el mismo problema:

```{r}
ggplot(sims_post_pred_tbl |> group_by(.draw, valor) |> count() |> 
         group_by(valor) |> 
         summarise(mediana = median(n), q_10 = quantile(n, 0.1), q90 = quantile(n, 0.9)) |> 
         pivot_longer(cols = c(mediana, q_10, q90), names_to = "tipo", values_to = "resumen"),
  aes(x = valor)) +
  geom_line(aes(y = resumen, group = tipo)) +
  labs(subtitle = "Distribución predictiva a priori") +
  ylab("Frecuencia") +
  geom_histogram(data = datos_obs, aes(x = y), bins = 30, alpha = 0.5) 
```


## Un segundo intento

Supongamos que después de investigar, nos enteramos que es común que 
algunos detectores fallen o estén defectuosos. En ese caso, marcan cero. Nótese
que la situación sería diferente, por ejemplo, si los detectores que se desbordan
marcan cero, etc. Es necesario regresar entonces al *análisis conceptual*, y repetir
todo el proceso.

Nuestros siguientes pasos dependen de que podamos entender cuál es la razón
del exceso de ceros con respecto a nuestro modelo inicial. En este caso,
tenemos que considerar que hay cierta probabilidad de que los detectores fallen.

Consideramos entonces el siguiente diagrama, donde $F$ representa el origen de los sensores
(o el lote de sensores):

```{r}
#| out-width: 100%
#| code-fold: true
grViz("
digraph {
  graph [ranksep = 0.3, rankdir = LR]
  node [shape=circle]
    L
    F
  node [shape=plaintext]
    y_j
  edge [minlen = 3]
    L -> y_j
    F_j ->  y_j
    F -> F_j
{rank = same; L;F}
   
}
")#, width = 200, height = 50)
```

Proponemos entonces un modelo como sigue: $y_n = 0$ con probabilidad $\pi$ (si $F_n=1$), y $y_n\sim \text{Poisson}(\lambda)$ con probabilidad $1-\pi$ ($f_n=0$). El modelo de datos se puede escribir 
como sigue:

$$p(y|\lambda, \pi) = (1-\pi) \textrm{pois}(y|\lambda) + \pi I(y=0)$$
que es una Poisson reescalada con una masa $\pi$ en cero (modelo Poisson con ceros inflados).


Recorremos los pasos con nuestro nuevo modelo, y consideraremos qué es lo que
sucede en la calibración algorítmica:


```{r}
N <- 100
R <- 1500
sim_datos <- list(N = N)

mod_ensemble <- cmdstan_model("src/flujo-mb/2-simular-ensemble.stan")
print(mod_ensemble)
sims_priori <- mod_ensemble$sample(data = sim_datos, 
  iter_sampling = R, chains = 1, refresh = R, seed = 4838282,
  fixed_param = TRUE)
```

Ahora podemos examinar algunas posibles configuraciones del modelo junto con las
observaciones que esperaríamos ver:

```{r}
#| warning: false
#| message: false
sims_tbl <-  sims_priori$draws(format = "df")
obs_priori_tbl <-  sims_tbl |> 
  as_tibble() |>
  pivot_longer(cols = starts_with("y"), names_to = "y", values_to = "valor") |> 
  separate(y, into = c("y", "n"), sep = "[\\[\\]]") |> select(-y)
ggplot(obs_priori_tbl |> filter(.draw < 5), aes(x = valor)) +
  geom_histogram(bins = 30) +
  facet_wrap(~lambda) +
  labs(subtitle = "Simulaciones de observaciones a priori")
```
Y podemos resumir estas simulaciones como

```{r}
ggplot(obs_priori_tbl |> group_by(.draw, valor) |> count() |> 
         group_by(valor) |> 
         summarise(mediana = median(n), q_10 = quantile(n, 0.1), q90 = quantile(n, 0.9)) |> 
         pivot_longer(cols = c(mediana, q_10, q90), names_to = "tipo", values_to = "resumen"),
  aes(x = valor)) +
  geom_line(aes(y = resumen, group = tipo)) +
  labs(subtitle = "Simulaciones de observaciones a priori")
```
Y vemos que es muy poco probable observar cantidades medidas por arriba de 25:

```{r}
obs_priori_tbl |> mutate(mayor_25 = valor > 25) |> 
  summarise(mayor_25 = mean(mayor_25)) 
```

## Calibración algorítmica

Bajo los supuestos del modelo, ahora podemos proponer nuestro algoritmo de estimación,
y checar en primer lugar que funciona apropiadamente. En este ejemplo, tomaremos
solamente 40 simulaciones y ajustaremos en cada caso el siguiente consecuencia de nuestros supuestos. Usualmente podemos usar 100 o más.

En este paso podemos ajustar nuestro muestreador, número de cadenas y su longitud, etc.

```{r}
#| message: false
mod_2 <- cmdstan_model("src/flujo-mb/2-modelo-poisson-cero-inflado.stan")
print(mod_2)
ajustes_apriori <- purrr::map(1:40, function(rep){
  y <- obs_priori_tbl |> filter(.draw == rep) |> pull(valor)
  datos_sim_lst <- list(y = y, N = length(y))
  ajuste <- mod_2$sample(data = datos_sim_lst, 
  iter_sampling = 1000, chains = 3, parallel_chains = 3, seed = 4838282, 
    refresh = 0, show_messages = FALSE)
  list(ajuste = ajuste, lambda_sim = sims_tbl$lambda[rep], p_sim = sims_tbl$p[rep])
})

```
Y vemos que nos encontramos con problemas. Examinamos los ajustes que producen divergencias:


```{r}
divergencias_lst <- ajustes_apriori |> map_dbl(
  ~ .x$ajuste$diagnostic_summary("divergences")$num_divergent |> sum()) 
div_sim <- which(divergencias_lst > 0)
div_sim
```

Veamos entonces que valores de $p$ y $lambda$ corresponden:

```{r}
diag_tbl <- obs_priori_tbl |> as_tibble() |> select(lambda, p, .draw) |> unique() |> 
  filter(.draw <= 40) |> 
  mutate(problemas = .draw %in% div_sim) 
ggplot(diag_tbl, aes(x = lambda, y = p, color = problemas, size = problemas)) +
  geom_point() +
  labs(subtitle = "Problemas de divergencia")
```

Y el problema aparece con valores extremos de $p$ y valores chicos de $\lambda$
(prueba haciendo más ajustes). Cuando estos valores se presentan, 
tenemos observaciones **que son principalmente ceros**, y es difícil distinguir
entre ceros que se deben a fallas en los detectores y ceros que se deben a una
tasa baja de Poisson. 

Este es un ejemplo donde vimos divergencias:

```{r}
filter(obs_priori_tbl |> filter(.draw == 17)) |> 
  select(valor) |> summarise(num_ceros = sum(valor == 0), num_no_ceros = sum(valor > 0))
```


Puedes ver más de esto en la discusión de  M. Betancourt en 
[aquí](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow#42_Second_Iteration:_If_At_First_You_Don't_Succeed,_Try_Try_Again).

## Tercer intento

En este punto, es necesario otra vez regresar al análisis conceptual: **los datos
no tienen información acerca de las causas de los ceros**. En primer
lugar, podríamos informarnos acerca de qué tan común es que los detectores fallen
(o si han existido chequeos recientes que nos den confianza que una proporción razonable
está funcionando), o adicionalmente cuál es el rango de tasas mínimas razonables
que se espera para el tipo de fuente con el que se está experimentando. En este caso,
podríamos evitar las zonas degeneradas y mal identificadas:

- Poniendo una inicial en la tasa de Poisson que no sea muy cercana a cero (para un
detector que está funcionando), si tenemos información en este sentido
- Descartando probabilidades extremas de fallas de detección: puede ser muy factible
que algunos detectores fallen, pero no que la mayoría falle, y quizá tampoco esperamos
que todos estén en perfectas condiciones.

Supongamos que en este ejemplo, después de hacer investigaciones adicionales, que
con la fuente seleccionada, 
todos los sensores son configurados para detectar al menos un mínimo de partículas
(la fuente es suficientemente fuerte si el sensor está funcionando), y adicionalmente,
sabemos que por el proceso, es altamente improbable que todos los sensores tengan fallas
(que interpetaremos como que es muy poco probable tener más de 90% de los detectores fallando).

Esta información no proviene de los datos, sino de conocimiento de dominio, y nos permite
configurar más apropiadamente nuestro modelo para evitar este comportamiento patológico.

Revisamos la inicial que estábamos usando anteriormente con un chequeo de la inicial:

```{r}
# distribución nicial de p
ggplot(obs_priori_tbl |> select(lambda, p, .draw) |> unique() ,
       aes(x = p)) + 
  geom_histogram(bins = 30) +
  labs(subtitle = "Distribución inicial de p")
ggplot(obs_priori_tbl |> select(lambda, p, .draw) |> unique() ,
       aes(x = lambda)) + 
  geom_histogram(bins = 30) +
  labs(subtitle = "Distribución inicial de lambda")
```


Ninguna de estas coincide con nuestro conocimiento de área. 
Podemos utilizar una gamma para
la inicial de $\lambda$, y una beta para la inicial de $p$. 

Comenzamos recordando el límite de unas 15 unidades para para un aparato que
esté funcionando. Experimentando con algunos valores de la gamma (por ejemplo,
queremos aproximadamente la media en 5 como en nuestro ejemplo anterior,
y la media de una gamma(a,b) es $a/b$):

```{r}
ggplot(tibble(lambda = rgamma(10000, 4, 0.6)), aes(x = lambda)) +
  geom_histogram(bins = 60) +
  labs(subtitle = "Distribución inicial de lambda")
```
Y para la probabilidad de falla, podemos usar una beta(2, 4) que nos da

```{r}
ggplot(tibble(lambda = rbeta(10000, 2, 4)), aes(x = lambda)) +
  geom_histogram(bins = 60) +
  labs(subtitle = "Distribución inicial de lambda")
```
Ahora repetimos los chequeos a priori:

```{r}
N <- 100
R <- 1500
sim_datos <- list(N = N)

mod_ensemble <- cmdstan_model("src/flujo-mb/3-simular-ensemble.stan")
print(mod_ensemble)
sims_priori <- mod_ensemble$sample(data = sim_datos, 
  iter_sampling = R, chains = 1, refresh = R, seed = 4838282,
  fixed_param = TRUE)
```

Ahora podemos examinar algunas posibles configuraciones del modelo junto con las
observaciones que esperaríamos ver:

```{r}
#| warning: false
#| message: false
sims_tbl <-  sims_priori$draws(format = "df")
obs_priori_tbl <-  sims_tbl |> 
  as_tibble() |>
  pivot_longer(cols = starts_with("y"), names_to = "y", values_to = "valor") |> 
  separate(y, into = c("y", "n"), sep = "[\\[\\]]") |> select(-y)
ggplot(obs_priori_tbl |> filter(.draw < 5), aes(x = valor)) +
  geom_histogram(bins = 30) +
  facet_wrap(~lambda) +
  labs(subtitle = "Simulaciones de observaciones a priori")
```
Y podemos resumir estas simulaciones como

```{r}
ggplot(obs_priori_tbl |> group_by(.draw, valor) |> count() |> 
         group_by(valor) |> 
         summarise(mediana = median(n), q_10 = quantile(n, 0.1), q90 = quantile(n, 0.9)) |> 
         pivot_longer(cols = c(mediana, q_10, q90), names_to = "tipo", values_to = "resumen"),
  aes(x = valor)) +
  geom_line(aes(y = resumen, group = tipo)) +
  labs(subtitle = "Simulaciones de observaciones a priori")
```
Y vemos que es muy poco probable observar cantidades medidas por arriba de 25:

```{r}
obs_priori_tbl |> mutate(mayor_25 = valor > 25) |> 
  summarise(mayor_25 = mean(mayor_25)) 
```


## Calibración algorítmica (intento 3)

Bajo los supuestos del modelo, ahora podemos proponer nuestro algoritmo de estimación,
y checar en primer lugar que funciona apropiadamente. En este ejemplo, tomaremos
solamente 40 simulaciones y ajustaremos en cada caso el siguiente consecuencia de nuestros supuestos. Usualmente podemos usar 100 o más.

En este paso podemos ajustar nuestro muestreador, número de cadenas y su longitud, etc.

```{r}
#| message: false
mod_3 <- cmdstan_model("src/flujo-mb/3-modelo-poisson-cero-inflado.stan")
print(mod_3)
ajustes_apriori <- purrr::map(1:40, function(rep){
  y <- obs_priori_tbl |> filter(.draw == rep) |> pull(valor)
  datos_sim_lst <- list(y = y, N = length(y))
  ajuste <- mod_3$sample(data = datos_sim_lst, 
  iter_sampling = 1000, chains = 3, parallel_chains = 3, seed = 4838282, 
    refresh = 0, show_messages = FALSE)
  list(ajuste = ajuste, lambda_sim = sims_tbl$lambda[rep], p_sim = sims_tbl$p[rep])
})

```

Una vez que corregimos valores no factibles con las iniciales, el algoritmo funciona bien:


```{r}
divergencias_lst <- ajustes_apriori |> map_dbl(
  ~ .x$ajuste$diagnostic_summary("divergences")$num_divergent |> sum()) 
div_sim <- which(divergencias_lst > 0)
div_sim
```

Ahora vemos si recuperamos o no los parámetros de la simulación.
En primer lugar, calculamos el cuantil del valor verdadero para la posterior de la simulación:

```{r}
resumen_cuantiles_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  lambda_sim <- ajuste_lst$lambda_sim
  cuantiles_tbl <- ajuste_lst$ajuste$draws("lambda", format = "df") |> 
    mutate(menor = lambda_sim < lambda) |> 
    summarise(q_menor = mean(menor))
  cuantiles_tbl |> mutate(lambda_sim = lambda_sim)
})
```

Ahora podemos hacer una gráfica de cuantiles: si estamos recuperando correctamente
los parámetros, la distribución de los cuantiles de los valores verdaderos
en la posterior debe ser cercana a uniforme (pues si $y\sim F$, entonces 
$P(F(y)<t)) = t$ para cualquier $t$):

```{r}
resumen_cuantiles_tbl
ggplot(resumen_cuantiles_tbl, aes(sample = q_menor)) +
  geom_qq(distribution = stats::qunif) +
  geom_abline(slope = 1, intercept = 0) +
  labs(subtitle = "Recuperación de valores en la posterior")
```

En este caso, vemos que estamos recuperando adecuadamente los valores
que pusimos en la simulación. 

```{r}
resumen_cuantiles_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  p_sim <- ajuste_lst$p_sim
  cuantiles_tbl <- ajuste_lst$ajuste$draws("p", format = "df") |> 
    mutate(menor = p_sim < p) |> 
    summarise(q_menor = mean(menor))
  cuantiles_tbl |> mutate(p_sim = p_sim)
})
```

Ahora podemos hacer una gráfica de cuantiles: si estamos recuperando correctamente
los parámetros, la distribución de los cuantiles de los valores verdaderos
en la posterior debe ser cercana a uniforme (pues si $y\sim F$, entonces 
$P(F(y)<t)) = t$ para cualquier $t$):

```{r}
ggplot(resumen_cuantiles_tbl, aes(sample = q_menor)) +
  geom_qq(distribution = stats::qunif) +
  geom_abline(slope = 1, intercept = 0) +
  labs(subtitle = "Recuperación de valores en la posterior")
```


Y los diagnósticos son razonables, podemos recuperar correctamente los parámetros
de interés.

### Calibración inferencial 3

Ahora hacemos nuestro diagnóstico de cómo estamos aprendiendo de los datos (ver más arriba
en el paso 1 para explicaciones). Los diagnósticos de calibración inferencial son
satisfactorios:

```{r}
contraccion_z_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  lambda_sim <- ajuste_lst$lambda_sim
  sd_previa <- 3.33 # ver inicial de lambda
  post_media_sd_tbl <- ajuste_lst$ajuste$draws("lambda", format = "df") |> 
    summarise(media_post = mean(lambda), sd_post = sd(lambda))
  tibble(contraccion = 1 - post_media_sd_tbl$sd_post^2/sd_previa^2,
         z = (lambda_sim - post_media_sd_tbl$media_post)/post_media_sd_tbl$sd_post)
})
```

```{r}
ggplot(contraccion_z_tbl, aes(x = contraccion, y = z)) +
  geom_point() +
  xlab("Contracción") + ylab("Valor z") + 
  xlim(0, 1) + labs(subtitle = "Lambda")
```


```{r}
contraccion_z_tbl <- ajustes_apriori |> map_df(function(ajuste_lst){
  p_sim <- ajuste_lst$p_sim
  sd_previa <- 0.182 # ver inicial de lambda
  post_media_sd_tbl <- ajuste_lst$ajuste$draws("p", format = "df") |> 
    summarise(media_post = mean(p), sd_post = sd(p))
  tibble(contraccion = 1 - post_media_sd_tbl$sd_post^2/sd_previa^2,
         z = (p_sim - post_media_sd_tbl$media_post)/post_media_sd_tbl$sd_post)
})
```

```{r}
ggplot(contraccion_z_tbl, aes(x = contraccion, y = z)) +
  geom_point() +
  xlab("Contracción") + ylab("Valor z") + 
  xlim(0, 1) + labs(subtitle = "Probabilidad p")
```


Los diagnósticos no muestran problemas. 

## Chequeos predictivos posteriores 3

Ajustamos los datos y checamos diagnósticos de la cadena, que no señalan problemas:



```{r}
datos_obs <- read_csv("../datos/ejemplo-flujo.csv")
datos_lst <- list(y = datos_obs$y, N = nrow(datos_obs))
  ajuste <- mod_3$sample(data = datos_lst, 
  iter_sampling = 1000, refresh = 1000, chains = 3, parallel_chains = 3, seed = 282)
```

```{r}
ajuste$summary(c("lambda","p")) |> 
select(mean, sd, q5, q95, rhat, ess_bulk, ess_tail)
```
Notemos cómo ahora la interpretación de $\lambda$ es diferente, y nuestra inferencia es que entre
0.23 y 0.38 de los detectores no están funcionando correctamente.


Ahora simulamos datos observados de la predictiva posterior ajustada, y comparamos con
los datos observados.

```{r}
sims_post_pred_tbl <- ajuste$draws("y_sim", format = "df") |> 
  as_tibble() |>
  pivot_longer(cols = starts_with("y_sim"), names_to = "y", values_to = "valor") |> 
  separate(y, into = c("y_sim", "n"), sep = "[\\[\\]]", extra = "drop") |> select(-y_sim)
```

```{r}
ggplot(sims_post_pred_tbl |> filter(.draw <=10) |> 
  bind_rows(datos_obs |> rename(valor = y) |> mutate(.draw = 11)), 
    aes(x = valor)) +
  geom_histogram(bins = 30) +
  facet_wrap(~.draw) +
  labs(subtitle = "Chequeo predictivo posterior")
```
Este diagnóstico se ve mucho mejor, ahora que hemos incluido la posibilidad de
detectores defectuosos.

Podemos hacer también hacer una gráfica agregada de la posterior predictiva,
comparando con la observada. Hasta ahora, este es el mejor resultado que hemos obtenido:

```{r}
ggplot(sims_post_pred_tbl |> group_by(.draw, valor) |> count() |> 
         group_by(valor) |> 
         summarise(mediana = median(n), q_10 = quantile(n, 0.1), q90 = quantile(n, 0.9)) |> 
         pivot_longer(cols = c(mediana, q_10, q90), names_to = "tipo", values_to = "resumen"),
  aes(x = valor)) +
  geom_line(aes(y = resumen, group = tipo)) +
  labs(subtitle = "Distribución predictiva a priori") +
  ylab("Frecuencia") +
  geom_histogram(data = datos_obs, aes(x = y), bins = 30, alpha = 0.5) 
```
## Cuarta iteración

Después de descubrir todos estos problemas con los datos, tenemos algo de preocupación
de que algo más esté pasando y que nuestros resultados no sean correctos. Después
de leer el manual de los sensores, nos damos cuenta de que tienen un punto de corte de
14 unidades, de forma que cuando se detectan más de 14 unidades, el sensor
devuelve un resultado nan. Sin embargo, para hacernos la vida "fácil", nos dicen que en esos
casos repitieron la medición del sensor hasta obtener algún valor.

Esto puede cambiar la inferencia, porque los datos están truncados por la derecha en 14.

Los que hicieron el experimento de medición nos diceen

Esto **no lo hacemos viendo los datos**, lo hacemos porque sabemos que el proceso
generador contiene características no modeladas que pueden afectar la inferencia.

En este punto, puedes consultar la fuente original de este caso de estudio
[M. Betancourt](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow) para
todos los chequeos. En este caso, sólo mostraremos cómo queda el modelo final y 
las inferencias obtenidas.


```{r}
#| message: false
mod_4 <- cmdstan_model("src/flujo-mb/4-modelo-poisson-cero-inflado-truncado.stan")
print(mod_4)
```

```{r}
datos_obs <- read_csv("../datos/ejemplo-flujo.csv")
datos_lst <- list(y = datos_obs$y, N = nrow(datos_obs))
  ajuste <- mod_4$sample(data = datos_lst, 
  iter_sampling = 1000, refresh = 1000, chains = 3, parallel_chains = 3, seed = 282)
```

```{r}
ajuste$summary(c("lambda","p")) |> 
select(mean, sd, q5, q95, rhat, ess_bulk, ess_tail)
```
